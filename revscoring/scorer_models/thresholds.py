from collections import defaultdict

import tabulate


class Thresholds(dict):

    def __init__(self, score_labels, label_rates, max_thresholds=200):
        """
        Construct a new threshhold object from the output of tests

        :Parameters:
            score_labels : `list` ( (`dict`, mixed) )
                A list of score documents generated by a scorer model and the
                corresponding labels.
            label_rates : `dict`
                A mapping of label classes with float representing the sampling
                rate in the test set.  `1.0` means that we should expect to see
                this label appear at the same rate as in the general
                population.  `2.0` means that we see this label at twice the
                rate of the general population, so it should be scaled
                down.  `0.5` means that we see this label at half the rate of
                general population, so it should be scaled up
                appropriately.
        """
        non_label_rates = self._build_non_label_rates(
            score_labels, label_rates)

        # For each label, build a set of stats
        for label, label_rate in label_rates.items():
            self[label] = self._build_stats(
                score_labels, label, label_rate, non_label_rates[label],
                max_thresholds)

    def __str__(self):
        s = ""
        for label, stats in self.items():
            s += str(label) + ": \n"
            table = []
            for stat in sorted(stats, key=lambda s: s['threshold']):
                table.append([stat['threshold'],
                              stat['accuracy'],
                              stat['recall'],
                              stat['!recall'],
                              stat['precision'],
                              stat['!precision'],
                              stat['f1'],
                              stat['!f1']])
            table_string = tabulate.tabulate(
                table, headers=['threshold', 'accuracy', 'recall', '!recall',
                                'precision', '!precision', 'f1', '!f1'],
                floatfmt=".3f")
            s += ''.join("\t" + line + "\n"
                         for line in table_string.split('\n'))
            s += "\n"

        return s

    @staticmethod
    def _build_non_label_rates(score_labels, label_rates):
        # Count up rates of observation and get the rescaled population rates
        # This math allows us to re-scale recall, precision, etc. later.

        # Count up the count of observations in the sample
        sample_label_counts = defaultdict(int)
        for s, label in score_labels:
            sample_label_counts[label] += 1

        # Convert the counts to a rate for each label
        sample_label_rates = {}
        for label, count in sample_label_counts.items():
            sample_label_rates[label] = count / len(score_labels)

        # Generate a rate for the group of observations that do not correspond
        # to each label
        non_label_rates = {}
        for label, sample_rate in sample_label_rates.items():
            non_label_rates[label] = sum(label_rates[l] * sample_label_rates[l]
                                         for l in sample_label_rates
                                         if l != label) / \
                                     sum(sample_label_rates[l]
                                         for l in sample_label_rates
                                         if l != label)

        return non_label_rates

    @staticmethod
    def _build_stats(score_labels, label, label_rate, non_label_rate,
                     max_thresholds):
        """
        Builds a set of threshold stats for a single label.
        """
        probas = (s['probability'][label] for s, l in score_labels)
        thresholds = Thresholds._limit_thresholds(probas, max_thresholds)

        threshold_stats = []
        for threshold in thresholds:
            y_preds = [s['probability'][label] >= threshold
                       for s, l in score_labels]
            y_trues = [l == label for s, l in score_labels]

            # Generate counts
            true_positives = 0
            false_positives = 0
            true_negatives = 0
            false_negatives = 0
            for y_pred, y in zip(y_preds, y_trues):
                true_positives += y_pred and y
                false_positives += y_pred and not y
                true_negatives += not y_pred and not y
                false_negatives += not y_pred and y

            # Apply scaling
            true_positives = true_positives / label_rate
            false_negatives = false_negatives / label_rate
            true_negatives = true_negatives / non_label_rate
            false_positives = false_positives / non_label_rate

            # Useful variables
            n = true_positives + true_negatives + \
                false_positives + false_negatives
            positives = true_positives + false_positives
            negatives = true_negatives + false_negatives
            trues = true_positives + false_negatives
            falses = false_positives + true_negatives
            true_predictions = true_positives + true_negatives

            # Generate stats
            accuracy = true_predictions / n
            recall = true_positives / max(trues, 1)
            _recall = true_negatives / max(falses, 1)
            precision = true_positives / max(positives, 1)
            _precision = true_negatives / max(negatives, 1)
            f1 = (precision * recall) / max(precision + recall, 1)
            _f1 = (_precision * _recall) / max(_precision + _recall, 1)

            # Build the record
            threshold_stats.append({
                'threshold': threshold,
                'accuracy': accuracy,
                'recall': recall,
                '!recall': _recall,
                'precision': precision,
                '!precision': _precision,
                'f1': f1,
                '!f1': _f1
            })
        return threshold_stats

    @staticmethod
    def _limit_thresholds(decision, max_thresholds):
        """
        Limits the range of a decision function (usually a probability) to
        a representative set of thresholds.
        """
        unique_thresholds = sorted(set(decision))
        if len(unique_thresholds) < max_thresholds:
            thresholds = unique_thresholds
        else:
            step = len(unique_thresholds) / max_thresholds
            float_i = 0
            thresholds = []
            while float_i <= len(unique_thresholds) - 2:
                thresholds.append(unique_thresholds[int(float_i)])
                float_i += step
            thresholds.append(unique_thresholds[-1])
        return thresholds
